{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Life insurance demonstration \u00b6 This repository includes a simple example of how to integrate with an existing MQ based framework that manage message distribution between MQ applications, in life insurance domain, to plug eventing capabilities like IBM Event Streams (Kafka) with minimum disruption. Architecture context \u00b6 Existing solution integrate a queueing framework that receive message from different applications (front end / mobile for the most part) in a Life insurance domain. This framework helps to support error management, retry, notification, data transformation, data life cycle and governance. The ask is to see how event-driven architecture will help to support some of the capability of the framework or complement it. Figure below illustrate a generic view of how the existing framework is running: At the top we have different front end applications that can send transactional data (write to life insurance model), or not transactional The APIs consumed by the front end could be mediated with ESB (IBM IIB) flows, and then some are publishing messages to IBM MQ queues. From those queues, we can get different processing running all together to do data enrichment, transformation, to get subscriber applications consuming those data. Other services are responsible to do retries, auditing, manage errors, or notify end user with a mobile push or email back-end An important component of this framework is the transaction event sourcing capability: keep state of change on some interesting transactional data: for example a life insurance offer. Different flows are doing the needed works, and all this framework is basically supporting long running transaction processing and notification engine. Also note, that to be generic this framework defines different mesage type (600) and adapt mediation flow via configuration. The solution in on bare metal or VM running on premise. Requirements to demonstrate \u00b6 Address how to extend existing architecture with Kafka based middleware and streaming processing. (See next section ) Demonstrate streaming processing with the exactly once delivery Ensure Event order is not changed: in the queuing approach with subscription, it is possible that a message arrived after another one could be processed before the first one is completed, which could impact data integrity. Demonstrate Data transformation to target different models, to prepare the data for a specific subscriber (a kafka consumer) Support message content based routing Dead letter queue support for data in error Support CloudEvent.io to present metadata around the message Support Schema management in registry to control the definition of the message in a unique central repository Demonstrate access control to topic Example of non-desruptive integration \u00b6 The existing framework can be extended by adding Kafka MQ source and sink connectors and then deploy Kafka based middleware (IBM Event Streams) so transactional or non-transactional data will be in different Kafka Topic. For the streaming processing, we propose to do data enrichment, data validation to route erranous data to dead-letter-queue, and data transformation to publish to two different topics for downstream subscribers: these will validate content based routing and enrichment, and exactly once delivery with order guarantee. Read more To understand Kafka topic - offset see this note Kafka MQ Source connector lab Domain model \u00b6 See the design section Components \u00b6 We will leverage the following IBM Products: Event Streams with one cluster definition is in this eventstreams-dev yaml MQ broker with AMQP protocol enabled (See this folder for deployment example) Kafka Connector Event end point management Schema registry And develop three components: A transaction simulator to send data to MQ to support different demonstration goals. The app is done in Java Messaging Service in lf-tx-simulator folder . a Kafka streaming processing using standard Java Kafka Streams API. The application is in client-event-processing folder Configuration for MQ source connector. The Yaml file is in environments mq-source folder More reading Building reactive Java apps with Quarkus and IBM MQ Quarkus AMQP 1.0 Quickstart","title":"Introduction"},{"location":"#life-insurance-demonstration","text":"This repository includes a simple example of how to integrate with an existing MQ based framework that manage message distribution between MQ applications, in life insurance domain, to plug eventing capabilities like IBM Event Streams (Kafka) with minimum disruption.","title":"Life insurance demonstration"},{"location":"#architecture-context","text":"Existing solution integrate a queueing framework that receive message from different applications (front end / mobile for the most part) in a Life insurance domain. This framework helps to support error management, retry, notification, data transformation, data life cycle and governance. The ask is to see how event-driven architecture will help to support some of the capability of the framework or complement it. Figure below illustrate a generic view of how the existing framework is running: At the top we have different front end applications that can send transactional data (write to life insurance model), or not transactional The APIs consumed by the front end could be mediated with ESB (IBM IIB) flows, and then some are publishing messages to IBM MQ queues. From those queues, we can get different processing running all together to do data enrichment, transformation, to get subscriber applications consuming those data. Other services are responsible to do retries, auditing, manage errors, or notify end user with a mobile push or email back-end An important component of this framework is the transaction event sourcing capability: keep state of change on some interesting transactional data: for example a life insurance offer. Different flows are doing the needed works, and all this framework is basically supporting long running transaction processing and notification engine. Also note, that to be generic this framework defines different mesage type (600) and adapt mediation flow via configuration. The solution in on bare metal or VM running on premise.","title":"Architecture context"},{"location":"#requirements-to-demonstrate","text":"Address how to extend existing architecture with Kafka based middleware and streaming processing. (See next section ) Demonstrate streaming processing with the exactly once delivery Ensure Event order is not changed: in the queuing approach with subscription, it is possible that a message arrived after another one could be processed before the first one is completed, which could impact data integrity. Demonstrate Data transformation to target different models, to prepare the data for a specific subscriber (a kafka consumer) Support message content based routing Dead letter queue support for data in error Support CloudEvent.io to present metadata around the message Support Schema management in registry to control the definition of the message in a unique central repository Demonstrate access control to topic","title":"Requirements to demonstrate"},{"location":"#example-of-non-desruptive-integration","text":"The existing framework can be extended by adding Kafka MQ source and sink connectors and then deploy Kafka based middleware (IBM Event Streams) so transactional or non-transactional data will be in different Kafka Topic. For the streaming processing, we propose to do data enrichment, data validation to route erranous data to dead-letter-queue, and data transformation to publish to two different topics for downstream subscribers: these will validate content based routing and enrichment, and exactly once delivery with order guarantee. Read more To understand Kafka topic - offset see this note Kafka MQ Source connector lab","title":"Example of non-desruptive integration"},{"location":"#domain-model","text":"See the design section","title":"Domain model"},{"location":"#components","text":"We will leverage the following IBM Products: Event Streams with one cluster definition is in this eventstreams-dev yaml MQ broker with AMQP protocol enabled (See this folder for deployment example) Kafka Connector Event end point management Schema registry And develop three components: A transaction simulator to send data to MQ to support different demonstration goals. The app is done in Java Messaging Service in lf-tx-simulator folder . a Kafka streaming processing using standard Java Kafka Streams API. The application is in client-event-processing folder Configuration for MQ source connector. The Yaml file is in environments mq-source folder More reading Building reactive Java apps with Quarkus and IBM MQ Quarkus AMQP 1.0 Quickstart","title":"Components"},{"location":"demo/","text":"Demonstration Script \u00b6 Goal \u00b6 The goal of the demonstration is to send two messages to illustrate client creation and update message in this order. Then each message is enriched and routed to different topics. The sequence of business operations: Bob TheBuilder is a new client so a web app is creating the record via a POST operation to the client management simulator The client record is updated by adding a beneficiary as spouce Then email address is changed. We can do two type of environment: Local on your laptop using docker compose Once deployed on OpenShift with Event Streams and MQ Local execution demonstration \u00b6 Pre-requisites You have docker desktop or similar installed on your laptop. Under the project starts docker compose: docker-compose up -d Verify the existing topics are empty chrome http://localhost:9000/ Configure MQ Source connector cd environments/local ./sendMQSrcConfig.sh Send a new client creation command # under home ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-bob.json OpenShift Deployment demonstration \u00b6 Pre-requisites You have te make tool Deploy the solution with one commmand cd environments make all See more detail in this section Verify your environments Verify MQ source connector is ready oc get kafkaconnectors # Output> # NAME CLUSTER CONNECTOR CLASS MAX TASKS READY # mq-source eda-kconnect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True Deeper view of the connector oc describe kafkaconnector mq-source Access the Simulator App Access the Simulator APIs by clicking on q/swagger-ui link from the home page: Use the POST on /api/v1/clients to send a client creation command with the following data: \"id\" : \"101012\" , \"code\" : \"C02\" , \"insuredPerson\" : { \"id\" : 2 , \"code\" : \"P02\" , \"first_name\" : \"julie\" , \"last_name\" : \"thesimmer\" , \"address\" : \"10 market street, CA, San Franciso\" , \"phone\" : \"650-650-650\" , \"mobile\" : \"\" , \"email\" : \"jswimmer@email.com\" }, \"client_category_id\" : 0 The request: The response with a populated id: Verify the message reaches the Kafka topic named lf-raw-tx","title":"Demonstration"},{"location":"demo/#demonstration-script","text":"","title":"Demonstration Script"},{"location":"demo/#goal","text":"The goal of the demonstration is to send two messages to illustrate client creation and update message in this order. Then each message is enriched and routed to different topics. The sequence of business operations: Bob TheBuilder is a new client so a web app is creating the record via a POST operation to the client management simulator The client record is updated by adding a beneficiary as spouce Then email address is changed. We can do two type of environment: Local on your laptop using docker compose Once deployed on OpenShift with Event Streams and MQ","title":"Goal"},{"location":"demo/#local-execution-demonstration","text":"Pre-requisites You have docker desktop or similar installed on your laptop. Under the project starts docker compose: docker-compose up -d Verify the existing topics are empty chrome http://localhost:9000/ Configure MQ Source connector cd environments/local ./sendMQSrcConfig.sh Send a new client creation command # under home ./e2e/local/sendTxToSimulator.sh ./e2e/data/client-bob.json","title":"Local execution demonstration"},{"location":"demo/#openshift-deployment-demonstration","text":"Pre-requisites You have te make tool Deploy the solution with one commmand cd environments make all See more detail in this section Verify your environments Verify MQ source connector is ready oc get kafkaconnectors # Output> # NAME CLUSTER CONNECTOR CLASS MAX TASKS READY # mq-source eda-kconnect-cluster com.ibm.eventstreams.connect.mqsource.MQSourceConnector 1 True Deeper view of the connector oc describe kafkaconnector mq-source Access the Simulator App Access the Simulator APIs by clicking on q/swagger-ui link from the home page: Use the POST on /api/v1/clients to send a client creation command with the following data: \"id\" : \"101012\" , \"code\" : \"C02\" , \"insuredPerson\" : { \"id\" : 2 , \"code\" : \"P02\" , \"first_name\" : \"julie\" , \"last_name\" : \"thesimmer\" , \"address\" : \"10 market street, CA, San Franciso\" , \"phone\" : \"650-650-650\" , \"mobile\" : \"\" , \"email\" : \"jswimmer@email.com\" }, \"client_category_id\" : 0 The request: The response with a populated id: Verify the message reaches the Kafka topic named lf-raw-tx","title":"OpenShift Deployment demonstration"},{"location":"deployment/","text":"OpenShift Deployment \u00b6 We assume you have already Cloud Pak for Integration installed, and you are already logged to the OpenShift console. Pre-requisites \u00b6 Using make \u00b6 We have a makefile to drive the installation of the demonstration components cd environments make all Example of output namespace/lf-demo created serviceaccount/lf-demo-sa created clusterrolebinding.rbac.authorization.k8s.io/secrets-to-sa created job.batch/cpsecret created --------------------------- ls-demo project created. Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . ------------------------------------------------ Create dev Event Streams cluster in lf-demo project ------------------------------------------------ eventstreams.eventstreams.ibm.com/dev created kafkatopic.eventstreams.ibm.com/lf-raw-tx created kafkatopic.eventstreams.ibm.com/lf-tx-a created kafkatopic.eventstreams.ibm.com/lf-tx-b created kafkauser.eventstreams.ibm.com/scram-user created kafkauser.eventstreams.ibm.com/tls-user created ------------------------------------------------ Create dev IBM MQ in lf-demo project ------------------------------------------------ configmap/mq-config created configmap/mq-mqsc-config created queuemanager.mq.ibm.com/lf-demo-mq created ------------------------------------------------ Create dev Kafka Connect cluster in lf-demo project ------------------------------------------------ kafkaconnect.eventstreams.ibm.com/eda-kconnect-cluster created ------------------------------------------------ Deploy MQ Source connector ------------------------------------------------ kafkaconnector.eventstreams.ibm.com/mq-source created configmap/lf-tx-simulator-cm created service/lf-tx-simulator created deployment.apps/lf-tx-simulator created route.route.openshift.io/lf-tx-simulator created configmap/lf-client-agent-cm created service/lf-client-agent created deployment.apps/lf-client-agent created route.route.openshift.io/lf-client-agent created","title":"OpenShift Deployment"},{"location":"deployment/#openshift-deployment","text":"We assume you have already Cloud Pak for Integration installed, and you are already logged to the OpenShift console.","title":"OpenShift Deployment"},{"location":"deployment/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"deployment/#using-make","text":"We have a makefile to drive the installation of the demonstration components cd environments make all Example of output namespace/lf-demo created serviceaccount/lf-demo-sa created clusterrolebinding.rbac.authorization.k8s.io/secrets-to-sa created job.batch/cpsecret created --------------------------- ls-demo project created. Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . Using project \"lf-demo\" on server \"https://api.ahsoka.coc-ibm.com:6443\" . ------------------------------------------------ Create dev Event Streams cluster in lf-demo project ------------------------------------------------ eventstreams.eventstreams.ibm.com/dev created kafkatopic.eventstreams.ibm.com/lf-raw-tx created kafkatopic.eventstreams.ibm.com/lf-tx-a created kafkatopic.eventstreams.ibm.com/lf-tx-b created kafkauser.eventstreams.ibm.com/scram-user created kafkauser.eventstreams.ibm.com/tls-user created ------------------------------------------------ Create dev IBM MQ in lf-demo project ------------------------------------------------ configmap/mq-config created configmap/mq-mqsc-config created queuemanager.mq.ibm.com/lf-demo-mq created ------------------------------------------------ Create dev Kafka Connect cluster in lf-demo project ------------------------------------------------ kafkaconnect.eventstreams.ibm.com/eda-kconnect-cluster created ------------------------------------------------ Deploy MQ Source connector ------------------------------------------------ kafkaconnector.eventstreams.ibm.com/mq-source created configmap/lf-tx-simulator-cm created service/lf-tx-simulator created deployment.apps/lf-tx-simulator created route.route.openshift.io/lf-tx-simulator created configmap/lf-client-agent-cm created service/lf-client-agent created deployment.apps/lf-client-agent created route.route.openshift.io/lf-client-agent created","title":"Using make"},{"location":"design/","text":"Solution Design \u00b6 Simple domain model for client \u00b6 We can use a simple client model to define a life insurance client that can have different benifeciary or transfer the life insurance to other persons. The model can be see as: Life insurance policy can be transferred to a family member or someone else, therefore the model stores not only information about the client to whom the policy belongs but also information about any related people and their relationship to the client. Client information is in Person objecr, but also as a Client . Other people related to the client to whom the policy may be transferred or who may receive the policy benefit upon the client\u2019s death are also Person s. Client Category is to be able to classify client for marketing reason based on demographics and financial details. The remaining two classes are needed for describing the nature of the relationship between clients and other people. Relation types is stored in the ClientRelationType . The ClientRelated instances store references to the client (client_id), the related person (person_id), the nature of that relation (client_relation_type_id), all addition details (details), if any, and a flag indicating whether the relation is currently active (is_active). The java classes for this model are in the lf-tx-simulator project. In the future, can extend this model with the Life Insurance offer and product. Model inspiration is coming from Vertabelo blog Transaction Simulator \u00b6 The code is under the folder lf-tx-simulator and use JMS API to interact with MQ. The main class is in MQProducer.java and expose a send(client) method with the domain class and prepare a transaction event message. The following code extract illustrates this: // this is more a demo trick. Should think of a better implementation TransactionEvent tx = new TransactionEvent (); if ( newClient ) { tx . type = TransactionEvent . TX_CLIENT_CREATED ; } else { tx . type = TransactionEvent . TX_CLIENT_UPDATED ; } tx . payload = client ; tx . txid = client . id ; tx . timestamp = new Date (). getTime (); String msg = parser . writeValueAsString ( tx ); TextMessage message = jmsContext . createTextMessage ( msg ); message . setJMSCorrelationID ( client . id ); producer . send ( destination , message ); The transaction event adds meta data and supports generic payload. We may replace it with CloudEvent . The code is using the JMSCorrelationID to define the potential Key to be used by Kafka producer. We will detail that in next section. The rest of this application exposes a REST resource to control the simulation and sends some new client data or update to existing client's one. The APIs should be enough to demonstrate the needed requirements. The deployment descriptors are in the environements/apps folder The MQ source connector \u00b6 The declaration of the Kafka connect cluster is done in the environements/service folder and the MQ source connector in kafka-mq-src-connector.yaml . The interesting part of the connector configuration is the use of JMS and the JMSCorrelationID as a source for the Kafka Record key. mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID The Client event stream processing. \u00b6 The code is in the client-event-processing folder The deployment descriptors are in the environements/apps folder","title":"Design"},{"location":"design/#solution-design","text":"","title":"Solution Design"},{"location":"design/#simple-domain-model-for-client","text":"We can use a simple client model to define a life insurance client that can have different benifeciary or transfer the life insurance to other persons. The model can be see as: Life insurance policy can be transferred to a family member or someone else, therefore the model stores not only information about the client to whom the policy belongs but also information about any related people and their relationship to the client. Client information is in Person objecr, but also as a Client . Other people related to the client to whom the policy may be transferred or who may receive the policy benefit upon the client\u2019s death are also Person s. Client Category is to be able to classify client for marketing reason based on demographics and financial details. The remaining two classes are needed for describing the nature of the relationship between clients and other people. Relation types is stored in the ClientRelationType . The ClientRelated instances store references to the client (client_id), the related person (person_id), the nature of that relation (client_relation_type_id), all addition details (details), if any, and a flag indicating whether the relation is currently active (is_active). The java classes for this model are in the lf-tx-simulator project. In the future, can extend this model with the Life Insurance offer and product. Model inspiration is coming from Vertabelo blog","title":"Simple domain model for client"},{"location":"design/#transaction-simulator","text":"The code is under the folder lf-tx-simulator and use JMS API to interact with MQ. The main class is in MQProducer.java and expose a send(client) method with the domain class and prepare a transaction event message. The following code extract illustrates this: // this is more a demo trick. Should think of a better implementation TransactionEvent tx = new TransactionEvent (); if ( newClient ) { tx . type = TransactionEvent . TX_CLIENT_CREATED ; } else { tx . type = TransactionEvent . TX_CLIENT_UPDATED ; } tx . payload = client ; tx . txid = client . id ; tx . timestamp = new Date (). getTime (); String msg = parser . writeValueAsString ( tx ); TextMessage message = jmsContext . createTextMessage ( msg ); message . setJMSCorrelationID ( client . id ); producer . send ( destination , message ); The transaction event adds meta data and supports generic payload. We may replace it with CloudEvent . The code is using the JMSCorrelationID to define the potential Key to be used by Kafka producer. We will detail that in next section. The rest of this application exposes a REST resource to control the simulation and sends some new client data or update to existing client's one. The APIs should be enough to demonstrate the needed requirements. The deployment descriptors are in the environements/apps folder","title":"Transaction Simulator"},{"location":"design/#the-mq-source-connector","text":"The declaration of the Kafka connect cluster is done in the environements/service folder and the MQ source connector in kafka-mq-src-connector.yaml . The interesting part of the connector configuration is the use of JMS and the JMSCorrelationID as a source for the Kafka Record key. mq.record.builder : com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder mq.connection.mode : client mq.message.body.jms : true mq.record.builder.key.header : JMSCorrelationID","title":"The MQ source connector"},{"location":"design/#the-client-event-stream-processing","text":"The code is in the client-event-processing folder The deployment descriptors are in the environements/apps folder","title":"The Client event stream processing."},{"location":"eda/","text":"Why event-driven architecture \u00b6 adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address. From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before. Technical needs \u00b6 At the technical level we can see three adoptions of event-driven solutions: Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026 Adopt asynchronous communication , publish-subscribe protocol between cloud-native microservices to help scaling and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. See more about EDA advantages for microservices Real time analytics : this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-based reasoning, or AI scoring integration on the data streams. Read more business requirements","title":"Why EDA now?"},{"location":"eda/#why-event-driven-architecture","text":"adoption for loosely coupled, event-driven microservice solutions, with new data pipeline used to inject data to modern data lakes, and the adoption of event backbone technology like Apache Kafka, or Apache Pulsar. Event-driven architecture (EDA) is an architecture pattern that promotes the production, detection, consumption of, and reaction to events. It supports asynchronous communication between components and most of the time a pub/sub programming model. The adoption of microservices brings some interesting challenges like data consistency, contract coupling, and scalability that EDA helps to address. From the business value point of view, adopting this architecture helps to scale business applications according to workload and supports easy extension by adding new components over time that are ready to produce or consume events that are already present in the overall system. New real-time data streaming applications can be developed which we were not able to do before.","title":"Why event-driven architecture"},{"location":"eda/#technical-needs","text":"At the technical level we can see three adoptions of event-driven solutions: Modern data pipeline to move the classical batch processing of extract, transform and load job to real-time ingestion, where data are continuously visible in a central messaging backbone. The data sources can be databases, queues, or specific producer applications, while the consumers can be applications, streaming flow, long storage bucket, queues, databases\u2026 Adopt asynchronous communication , publish-subscribe protocol between cloud-native microservices to help scaling and decoupling: the adoption of microservices for developing business applications, has helped to address maintenance and scalability, but pure RESTful or SOAP based solutions have brought integration and coupling challenges that inhibited the agility promised by microservice architecture. Pub/sub helps to improve decoupling, but design good practices are very important. See more about EDA advantages for microservices Real time analytics : this embraces pure analytic computations like aggregate on the data streams but also complex event processing, time window-based reasoning, or AI scoring integration on the data streams. Read more business requirements","title":"Technical needs"}]}